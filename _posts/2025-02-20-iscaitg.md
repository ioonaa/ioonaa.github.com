---
layout: post

title: Workshop on Diversity in Large Speech and Language Models
---

The workshop took place at <strong>Humboldt University of Berlin</strong>. 
It was supported by the <strong>International Speech Communication Association</strong> (ISCA) and the <strong>Informationstechnische Gesellschaft</strong> (ITG), 
and set out to address topics such as:
 <ul>
  <li><strong>User diversity</strong>: Which aspects of human speech and language production affect the performance of large language models (LLMs)? In which way, and for which tasks?</li>
  <li><strong>Language use</strong>: How are LLMs able to cope with different languages, dialects, and sociolects? How do they deal with code switching?</li>
  <li><strong>Human adaptation</strong>: How does the use of LLMs affect language comprehension, as well as speech and language production? Which alignment effects occur, and in which time spans?</li>
  <li><strong>Model adaptation</strong>: How do models need to be designed to better cope with speech and language diversity? How do training and finetuning affect model performance?</li>
  <li><strong>Inclusion</strong>: What data and technologies are necessary to better cope with diversity in LLMs?</li>
 </ul> 
 
With my colleague <strong>Erfan Amirzadeh Shams</strong>, we presented our work on <strong>How Transformer-based ASR Models Handle Diverse Signal Perturbations at the Segmental Level</strong>.
I also participated in a panel discussion on future research directions that tackle diversity issues in LLMs. 


![Foto]({{ site.url }}/images/isca-itg25.jpg "Berliner Philharmonie, poster presentation, panel discussion"){:height="200" .center-image}
